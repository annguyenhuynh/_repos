{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Demo\n",
    "\n",
    "*   This project is done on the local machine by clonning git URL using homebrew on Macbook.\n",
    "\n",
    "1.   Install homebrew in your terminal: https://brew.sh/. Make sure you run all the recommended links.\n",
    "\n",
    "2.   Create a new repository on your Github site and copy the **HTTPS** url.\n",
    "\n",
    "3.   In the same terminal, type 'git clone' and paste the link:\n",
    "    e.g.: git clone https://github.com/annguyenhuynh/Text-Summarization-Project.git\n",
    "\n",
    "4.   On your local machine, you will see a folder with the name of the repository you created on Github that contains folders of your license (if you choose one) and README (if you also create one)\n",
    "\n",
    "5.   The IDE used in this project is VSCode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Project Structure**\n",
    "\n",
    "*   In VSCode, under the Folder created by cloning your Github, create a file called \"template.py\". This file contains the template layout for this project.\n",
    "\n",
    "*   The code below creating folders and files that will be used in the project\n",
    "\n",
    "*   After building the code, in the terminal, run **python3 template.py** and your folder structure will be automatically created for you on the local machine.\n",
    "\n",
    "*   You can create more folders and files later as needed by adding in the list 'list_files' and run the template.py in the terminal to make the updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n",
    "\n",
    "project_name = \"textSummarizer\"\n",
    "\n",
    "list_files = [\n",
    "  \".github/workflows/.gitkeep\",\n",
    "  f\"src/{project_name}/__init__.py\",\n",
    "  f\"src/{project_name}/components/__init__.py\",\n",
    "  f\"src/{project_name}/utils/__init__.py\"\n",
    "  f\"src/{project_name}/utils/common.py\",\n",
    "  f\"src/{project_name}/logging/__init__.py\",\n",
    "  f\"src/{project_name}/config/__init__.py\",\n",
    "  f\"src/{project_name}/config/configuration.py\",\n",
    "  f\"src/{project_name}/pipeline/__init__.py\",\n",
    "  f\"src/{project_name}/entity/__init__.py\",\n",
    "  f\"src/{project_name}/constants/__init__.py\",\n",
    "  \"config/config.yaml\",\n",
    "  \"params.yaml\",\n",
    "  \"app.py\",\n",
    "  \"main.py\",\n",
    "  \"Dockerfile\",\n",
    "  \"reqirements.txt\",\n",
    "  \"setup.py\",\n",
    "  \"research/trials.ipynb\"\n",
    "]\n",
    "\n",
    "for filepath in list_files:\n",
    "  filepath = Path(filepath)\n",
    "  filedir,filename = os.path.split(filepath)\n",
    "\n",
    "  if filedir != \"\":\n",
    "    os.makedirs(filedir,exist_ok=True)\n",
    "    logging.info(f\"Creating directory: {filedir} for file: {filename}\")\n",
    "\n",
    "  if(not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
    "    with open(filepath, 'w') as f:\n",
    "      pass\n",
    "    logging.info(f\"Creating empty file: {filepath}\")\n",
    "\n",
    "  else:\n",
    "    logging.info(f\"{filename} already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commit to Github**\n",
    "\n",
    "*  In your terminal, write and run\n",
    "    *     git add .\n",
    "    *     git commit -m \"folder structure added\"\n",
    "    *    git push origin main\n",
    "\n",
    "*   Go to Github site, refresh it and you will see all folders and files there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating environment for this project**\n",
    "\n",
    "*   We should create an environment whenever we start a ML or DL project because each project may require different libraries and packages and some have dependency issues so we want to have all the projects separated from one another\n",
    "\n",
    "*   How to create virtual environment?\n",
    "    *   conda create -n [venv_name] python=3.8(versioning depends on your choice) -y\n",
    "\n",
    "*   Why should we create conda env and not python env?\n",
    "    *   According to my search on Google, Conda environments are more specific and better for reproducibility. Conda combines environment and package management, which can reduce compatibility issues and streamline workflow. Conda environments are often used in data science because they can handle complex dependencies, especially in scientific computing libraries.\n",
    "\n",
    "*   After creating, you need to activate the virtual environment\n",
    "    *   conda activate [venv_name]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updating *requirements.txt***\n",
    "\n",
    "*   In this txt file, we will list all the packages we need to install for this specific project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "transformers\n",
    "transformers[sentencepiece]\n",
    "datasets\n",
    "sacrebleu\n",
    "rouge-score\n",
    "py7zr\n",
    "pandas\n",
    "nltk\n",
    "tqdm\n",
    "PyYAML\n",
    "matplotlib\n",
    "torch\n",
    "notebook\n",
    "boto3\n",
    "mypy-boto3-s3\n",
    "python-box==6.0.2\n",
    "ensure==1.0.2\n",
    "fastapi==0.78.0\n",
    "uvicorn==0.18.3\n",
    "Jinja2==3.1.2\n",
    "-e .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import setuptools\n",
    "\n",
    "with open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "  long_description = f.read()\n",
    "\n",
    "__version__ = \"0.0.0\"\n",
    "\n",
    "REPO_NAME = \"Text-Summarization-Project\"\n",
    "AUTHOR_USERNAME = \"annguyenhuynh\"\n",
    "SRC_REPO = \"textSummarizer\"\n",
    "AUTHOR_EMAIL = \"hnpa1997@gmail.com\"\n",
    "\n",
    "setuptools.setup(\n",
    "  name=SRC_REPO,\n",
    "  version=__version__,\n",
    "  author=AUTHOR_USERNAME,\n",
    "  author_email=AUTHOR_EMAIL,\n",
    "  description=\"Text summarization project using NLP techniques\",\n",
    "  long_description=long_description,\n",
    "  long_description_content_type=\"text/markdown\",\n",
    "  url=f\"https://github.com/{AUTHOR_USERNAME}/{REPO_NAME}\",\n",
    "  project_urls={\n",
    "    \"Bug Tracker\": f\"https://github.com/{AUTHOR_USERNAME}/{REPO_NAME}/issues\",\n",
    "  },\n",
    "  package_dir={\"\": \"src\"},\n",
    "  packages=setuptools.find_packages(where=\"src\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installing all the packages in the .txt file and commit to Github**\n",
    "\n",
    "*   pip install -r reqirements.txt\n",
    "\n",
    "*   Run the above command in your terminal in the conda virtual environment. Besides running the .txt file, it will also run the 'setup.py' file. Now, everything is ready for the project.\n",
    "\n",
    "*   Remember to commit to Github following the same steps used above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up the logging**\n",
    "\n",
    "*   src --> textSummarizer --> logging --> open '__init__.py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging_str = \"[%(asctime)s] %(levelname)s: %(module)s: %(message)s]\"\n",
    "log_dir = \"logs\"\n",
    "log_filepath = os.path.join(log_dir, \"running_logs.log\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "  level=logging.INFO,\n",
    "  format=logging_str,\n",
    "  handlers=[\n",
    "    logging.FileHandler(log_filepath),\n",
    "    logging.StreamHandler(sys.stdout)]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*   In **main.py**, run\n",
    "    *   from textSummarizer.logging import logger\n",
    "    *   logger.info(\"Welcome to our custom logging\")\n",
    "\n",
    "*   In the terminal, if you run **python3 main.py**, you will see something like this:\n",
    "    *   [2024-08-31 14:12:55,021] INFO: main: Welcome to our custom logging]\n",
    "\n",
    "*   The results returned follow the setup of the logging_str: timestamp level(info) module(main) and message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store all the functions needed in common.py**\n",
    "\n",
    "*   We will store all the packages needed in a file, and so when we need to use a specific function, we can refer to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "from box.exceptions import BoxValueError\n",
    "import yaml\n",
    "from textSummarizer.logging import logger\n",
    "from ensure import ensure_annotations\n",
    "from box import ConfigBox\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "@ensure_annotations\n",
    "def read_yaml(path_to_yaml:Path) -> ConfigBox:\n",
    "  \"\"\"read yaml file and return\n",
    "\n",
    "  Args:\n",
    "    path_to_yaml (str):path like input\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if yaml file is empty\n",
    "    e: empty file\n",
    "\n",
    "  Returns:\n",
    "    ConfigBox: ConfigBox type\n",
    "  \"\"\"\n",
    "  try:\n",
    "    with open(path_to_yaml) as yaml_file:\n",
    "      content = yaml.safe.load(yaml_file)\n",
    "      logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n",
    "      return ConfigBox(content)\n",
    "  except BoxValueError:\n",
    "    raise ValueError(\"yaml file is empty\")\n",
    "  except Exception as e:\n",
    "    raise e\n",
    "\n",
    "  @ensure_annotations\n",
    "  def create_directories(path_to_directories: list, verbose=True):\n",
    "    \"\"\"create list of directories\n",
    "\n",
    "    Args:\n",
    "      path_to_directories(list):list of path to directories\n",
    "      ignore_log (bool, optional): ignore if multiple directories are to be created, default to False.\n",
    "    \"\"\"\n",
    "    for path in path_to_directories:\n",
    "      os.makedirs(path, exist_ok=True)\n",
    "      if verbose:\n",
    "        logger.info(f\"Directory created at: {path}\")\n",
    "\n",
    "  @ensure_annotations\n",
    "  def get_size(path:Path) -> str:\n",
    "    \"\"\"get size in KB\n",
    "    Args:\n",
    "    path (Path): path to the file\n",
    "\n",
    "    Returns:\n",
    "    str: size in KB\n",
    "    \"\"\"\n",
    "    size_in_kb = round(os.path.getsize(path)/1024)\n",
    "    return f\"~ {size_in_kb} KB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'@': **decorator** with **ensure_annotations**\n",
    "\n",
    "*   A decorator in Python is a function that modifies or extends the behavior of another function\n",
    "without changing the original function's code. Decorators are a design pattern that can be used to enhance the functionality of classes, methods, or functions.\n",
    "\n",
    "*   Decorators can be used for logging and caching. Logging saves information about executed functions, such as arguments and return values. Caching stores arguments and return values so they can be reused\n",
    "\n",
    "*   For e.g: you define a function\n",
    "    *   def get_product(x: int, y: int) -> int:\n",
    "          return x*y\n",
    "  *  However, you call the function like this get_product(2,'4'). This returns '44' as string. If you add *@ensure_annotation* before your function, even 4 is entered as string, the output is still 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'->': **Hints**: tell the developers types of object expects and what it returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "' \"\"\" \"\"\" ': Everything inside these triple quotes are pseudo-code. It is the outline/logic of the real code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configbox**: read Python config values into Python types. We need it here because we have a function that reads YAML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textS/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/AnhHuynh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, load_metric\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Why CUDA GPU?\\nCUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA\\nParallel Computing: CUDA GPUs can run thousands of threads simultaneously, making them highly effective for tasks that can be parallelized.\\nAccelerated Workloads: Tasks like matrix operations, deep learning model training, and real-time image or video processing are much faster on a CUDA GPU compared to a CPU.\\nDeep Learning: CUDA is widely used in deep learning frameworks like TensorFlow and PyTorch to speed up model training and inference by utilizing the GPU.\\n\\nDevelopment Tools: CUDA provides libraries, tools, and frameworks (e.g., cuBLAS, cuDNN) to help developers write high-performance applications that can run on NVIDIA GPUs.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Why CUDA GPU?\n",
    "CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA\n",
    "Parallel Computing: CUDA GPUs can run thousands of threads simultaneously, making them highly effective for tasks that can be parallelized.\n",
    "Accelerated Workloads: Tasks like matrix operations, deep learning model training, and real-time image or video processing are much faster on a CUDA GPU compared to a CPU.\n",
    "Deep Learning: CUDA is widely used in deep learning frameworks like TensorFlow and PyTorch to speed up model training and inference by utilizing the GPU.\n",
    "\n",
    "Development Tools: CUDA provides libraries, tools, and frameworks (e.g., cuBLAS, cuDNN) to help developers write high-performance applications that can run on NVIDIA GPUs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code explain**\n",
    "\n",
    "*   **Model Checkpoint**: google/pegasus-cnn_dailymail. Pegasus pretraining task is intentionally similar to summarization: important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary\n",
    "\n",
    "*   **AutoTokenizer**: loads the tokenizer associated with the specified model checkpoint. The tokenizer is responsible for converting text into tokens that the model can understand and later converting model output tokens back into text.\n",
    "\n",
    "*   **AutoModelForSeq2SeqLM**: AutoModelForSeq2SeqLM can be used to load any seq2seq (or encoder-decoder) model that has a language modeling (LM) head on top\n",
    "\n",
    "*   [**Auto Classes**](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSeq2SeqLM)\n",
    "\n",
    "*   [**Hugging Face - Pegasus**](https://huggingface.co/google/pegasus-cnn_dailymail)\n",
    "\n",
    "*   [GenAI Udemy course](https://www.udemy.com/share/10bnQZ3@8z0IN-HKfHQ5jQHrBrD4Ipgm39VhFuZdEH4l4rwhoahc4MKvBJCTdPZl-RJT2y2tyQ==/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SAMSum dataset\n",
    "dataset = load_dataset(\"samsum\") \n",
    "\n",
    "# Display the first few examples from the train split\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset[split]) for split in dataset]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n",
    "print(\"\\nDialogue\")\n",
    "\n",
    "print(dataset['test'][1]['dialogue'])\n",
    "\n",
    "print('\\nSummary')\n",
    "\n",
    "print(dataset['test'][1]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code explain**\n",
    "\n",
    "\n",
    "*   split: The dataset object is typically a dictionary-like object where each key corresponds to a split (e.g., \"train\", \"validation\", \"test\").\n",
    "\n",
    "*   len(dataset[split]): For each split in the dataset, len() calculates the number of examples in that split.\n",
    "\n",
    "*   [len(dataset[split]) for split in dataset] iterates over all the splits in the dataset, calculates their lengths, and stores the results in a list.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "  input_encodings = tokenizer(example_batch['dialogue'], max_length = 1024, truncation=True)\n",
    "  \n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    target_encodings = tokenizer(example_batch['summary'],max_length = 128,truncation=True)\n",
    "\n",
    "  return {\n",
    "      'input_ids': input_encodings['input_ids'],\n",
    "      'attention_mask': input_encodings['attention_mask'],\n",
    "      'labels': target_encodings['input_ids']\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code explain**\n",
    "*   This function is used to prepocess text data for seq-to-seq task. This function takes a batch of examples  from the dataset loaded earlier and converts them into features that a model can use for training or inference.\n",
    "\n",
    "*   **tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)**: This line tokenizes the input text, which is the dialogue in this case. The max_length=1024 parameter ensures that the input sequence is truncated if it exceeds 1024 tokens. The result is a dictionary containing input_ids (the tokenized input) and attention_mask (which indicates which tokens are padding and which are actual input).\n",
    "\n",
    "*   **with tokenizer.as_target_tokenizer()**: This temporarily switches the tokenizer to target mode. This is important for seq2seq tasks where the model needs to generate text.\n",
    "\n",
    "*   **target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)**: This tokenizes the target text, which is the summary in this case, ensuring the sequence length does not exceed 128 tokens.\n",
    "\n",
    "*   Returning features:\n",
    "\n",
    "    *   **input_ids**: The token IDs corresponding to the input dialogue.\n",
    "\n",
    "    *   **attention_mask**: The attention mask that tells the model which parts of the input are actual tokens and which are padding.\n",
    "\n",
    "    *   **labels**: The token IDs corresponding to the target summary. These are used as the labels during training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/818 [00:00<?, ? examples/s]/opt/anaconda3/envs/textS/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [00:00<00:00, 2016.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(convert_examples_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer,model=model_pegasus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code explain**\n",
    "\n",
    "*   DataCollatorForSeq2Seq is a utility in the Hugging Face Transformers library designed to help with batching and preparing data for sequence-to-sequence (seq2seq) tasks. It is particularly useful when working with models like T5, BART, and Pegasus.\n",
    "\n",
    "*   When training or evaluating seq2seq models, you need to handle batches of input sequences and their corresponding target sequences (labels). These sequences often have different lengths, so padding is necessary to ensure that all sequences in a batch have the same length. DataCollatorForSeq2Seq automates this process, making it easier to prepare your data for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textS/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir = 'pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10,\n",
    "    evaluation_strategy ='steps', eval_steps=500, save_steps=16,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code explain**\n",
    "\n",
    "*   **output_dir**='pegasus-samsum': This specifies the directory where the model checkpoints, logs, and other outputs will be saved. After training, you'll find the fine-tuned model, configuration files, and any evaluation results in this directory.\n",
    "\n",
    "*   **num_train_epochs=1**: The number of complete passes through the training dataset. In this case, it's set to 1, meaning the model will be trained for one epoch.\n",
    "\n",
    "*   **warmup_steps=500**: The number of steps used for a warmup phase, where the learning rate increases linearly from 0 to its target value. This can help stabilize training in the early stage\n",
    "\n",
    "*   **weight_decay=0.01**: Weight decay is a regularization technique to prevent overfitting by penalizing large weights in the model. This value (0.01) is applied during optimization.\n",
    "\n",
    "*   **logging_steps=10**: This specifies that the training metrics (like loss, learning rate, etc.) should be logged every 10 steps.\n",
    "\n",
    "*   **evaluation_strategy='steps'**: This tells the Trainer to run evaluation during training based on steps. Other options include epoch (to evaluate at the end of each epoch) or no (no evaluation during training).\n",
    "\n",
    "*   **eval_steps=500**: Evaluation will be performed every 500 steps during training. This is useful for monitoring model performance throughout training.\n",
    "\n",
    "*   **save_steps=16**: This determines how often model checkpoints will be saved, set here to every 16 steps. Frequent checkpointing can help avoid losing progress in case of interruptions.\n",
    "\n",
    "\n",
    "*   **gradient_accumulation_steps=16**: Gradients will be accumulated over 16 steps before performing a backward pass. This effectively increases the batch size to 16 * per_device_train_batch_size, allowing you to simulate a larger batch size without requiring as much GPU memory.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=tokenized_dataset[\"test\"],\n",
    "                  eval_dataset=tokenized_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 10/51 [21:44<1:35:36, 139.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1101, 'grad_norm': 31.94026756286621, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [34:42<1:15:21, 129.18s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [42:55<1:02:48, 121.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0467, 'grad_norm': 15.841309547424316, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [1:03:52<45:15, 129.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1613, 'grad_norm': 10.095331192016602, 'learning_rate': 3e-06, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [1:08:12<41:01, 129.57s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [1:23:50<21:39, 118.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9901, 'grad_norm': 20.720611572265625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [1:37:45<05:14, 104.78s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [1:41:13<01:43, 103.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8539, 'grad_norm': 51.40227127075195, 'learning_rate': 5e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [1:42:51<00:00, 101.52s/it]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [1:43:06<00:00, 121.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6186.8641, 'train_samples_per_second': 0.132, 'train_steps_per_second': 0.008, 'train_loss': 3.0231569280811383, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51, training_loss=3.0231569280811383, metrics={'train_runtime': 6186.8641, 'train_samples_per_second': 0.132, 'train_steps_per_second': 0.008, 'total_flos': 313450454089728.0, 'train_loss': 3.0231569280811383, 'epoch': 0.9963369963369964})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "def generate_batch_sized_chunk(list_of_elements, batch_size):\n",
    "  \"\"\"split the dataset into smaller batches that we can process simultaneuosly. Yield successive batch-sized chunks from list of elements\"\"\"\n",
    "  for i in range(0, len(list_of_elements), batch_size):\n",
    "    yield list_of_elements[i:i + batch_size]\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric,model, tokenizer,\n",
    "                                batch_size=16, device=device,\n",
    "                                column_text = 'article',\n",
    "                                column_summary = 'highlights'):\n",
    "  article_batches = list(generate_batch_sized_chunk(dataset[column_text], batch_size))\n",
    "  target_batches = list(generate_batch_sized_chunk(dataset[column_text],batch_size)) \n",
    "\n",
    "  for article_batch, target_batch in tqdm(\n",
    "    zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "    inputs = tokenizer(article_batch,max_length=1024,truncation=True,\n",
    "                       padding=\"max_length\", return_tensors=\"pt\") \n",
    "\n",
    "    summaries = model.generate(input_ids=inputs['input_ids'].to(device), \n",
    "                               attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                               length_penalty=0.8, num_beams=8, max_length=128)\n",
    "    '''parameter for legnth penalty ensures that the model does not generate sequences that are too long.'''\n",
    "  #Decode generated text,\n",
    "  # replace the token, and add the decoded texts with the references to the metrics\n",
    "    decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                            clean_up_tokenization_spaces=True)\n",
    "                    for s in summaries] \n",
    "    decoded_summaries = [d.replace(\"\",\" \") for d in decoded_summaries]\n",
    "\n",
    "    metric.add_batch(predictions = decoded_summaries, references = target_batch)\n",
    "\n",
    "  # Compute and return the rogue scores\n",
    "  score = metric.compute() \n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code explain**\n",
    "* **tqdm**: provides a progress bar to track the loopâ€™s progress.\n",
    "* **zip**: pairs the batches of articles and target summaries for processing.\n",
    "* **tokenizer**: encodes the input texts (articles) into token IDs that the model can process. It also pads the sequences to a maximum length and returns them as PyTorch tensors.\n",
    "* **num_beams** (int, optional, defaults to 1) â€” Number of beams for beam search. 1 means no beam search.\n",
    "* *Beam Search* is a  search algorithm used in deep learning and natural language processing (NLP) that helps find solutions to problems with large search spaces.\n",
    "* **Length penalty** normalizes the scores based on the sequence length\n",
    "* **pt**: pytorch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.65kB [00:00, 1.85MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "rogue_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"] \n",
    "rogue_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROUGE SCORE**\n",
    "* The ROUGE score, or *Recall-Oriented Understudy for Gisting Evaluation*, is a metric used to evaluate the quality of machine translation and summarization models. It compares a machine-generated summary or translation to a human-produced reference. The ROUGE score is a scalar value between 0 and 1, with higher scores indicating greater similarity between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [09:41<00:00, 116.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus]</th>\n",
       "      <td>0.074672</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.074331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "pegasus]  0.074672  0.005143  0.067024   0.074331"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "  dataset['test'][0:10], rogue_metric, trainer.model, tokenizer, batch_size=2, column_text='dialogue', column_summary='summary')\n",
    "rogue_dict = dict((rn, score[rn].mid.fmeasure) for rn in rogue_names) \n",
    "# score[rn].mid.fmeasure, which is the F1 score for that particular ROUGE metric.\n",
    "\n",
    "pd.DataFrame(rogue_dict,index=[f'pegasus]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "# Save model \n",
    "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/spiece.model',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:  Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Reference Summary:  Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "Model Summary:  Amanda: Ask Larry Amanda: He called her last time we were at the park together .<n>Hannah: I'd rather you texted him .<n>Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "#Predictions \n",
    "\n",
    "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\": 8, \"max_length\": 128}\n",
    "sample_text = dataset[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset[\"test\"][0][\"summary\"] \n",
    "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\", tokenizer=tokenizer)\n",
    "\n",
    "print(\"Dialogue: \", sample_text)\n",
    "\n",
    "print(\"\\nReference Summary: \", reference)\n",
    "\n",
    "print(\"\\nModel Summary: \", pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
